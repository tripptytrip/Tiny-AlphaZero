{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # üß† TinyAlphaZero: A Minimal Chess Engine via Self-Play\n",
        "#\n",
        "# This notebook implements a simplified version of DeepMind's AlphaZero algorithm for chess.\n",
        "# The model learns to play chess **entirely from self-play** - no human games, no opening books,\n",
        "# no handcrafted evaluation functions.\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ## Architecture Overview\n",
        "#\n",
        "# ```\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ                      Board State (69 tokens)                    ‚îÇ\n",
        "# ‚îÇ  [64 squares: piece/empty] + [turn] + [4 castling rights]       ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "#                                  ‚îÇ\n",
        "#                                  ‚ñº\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ                   Token + Position Embeddings                    ‚îÇ\n",
        "# ‚îÇ  ‚Ä¢ 23-token vocabulary (pieces + flags)                         ‚îÇ\n",
        "# ‚îÇ  ‚Ä¢ Factored position encoding (rank + file, not flat)           ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "#                                  ‚îÇ\n",
        "#                                  ‚ñº\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ                    Transformer Encoder (6 layers)               ‚îÇ\n",
        "# ‚îÇ  ‚Ä¢ Full attention (every square sees every other square)        ‚îÇ\n",
        "# ‚îÇ  ‚Ä¢ No causal mask (this isn't autoregressive)                   ‚îÇ\n",
        "# ‚îÇ  ‚Ä¢ Pre-norm for training stability                              ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "#                                  ‚îÇ\n",
        "#                                  ‚ñº\n",
        "#                         Mean Pool (aggregate)\n",
        "#                                  ‚îÇ\n",
        "#                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "#                     ‚ñº                       ‚ñº\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ      Policy Head            ‚îÇ ‚îÇ      Value Head             ‚îÇ\n",
        "# ‚îÇ  \"What move to play?\"       ‚îÇ ‚îÇ  \"Who's winning?\"           ‚îÇ\n",
        "# ‚îÇ                             ‚îÇ ‚îÇ                             ‚îÇ\n",
        "# ‚îÇ  Output: 4096 logits        ‚îÇ ‚îÇ  Output: 3 classes          ‚îÇ\n",
        "# ‚îÇ  (64 from √ó 64 to squares)  ‚îÇ ‚îÇ  [Loss, Draw, Win]          ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "# ```\n",
        "#\n",
        "# ### Key Design Choices (vs Original AlphaZero)\n",
        "#\n",
        "# | Aspect | AlphaZero | TinyAlphaZero | Why |\n",
        "# |--------|-----------|---------------|-----|\n",
        "# | **Board encoding** | 119 spatial planes (8√ó8√ó119) | 69 tokens | Simpler, works with transformers |\n",
        "# | **Network** | ResNet (CNN) | Transformer | Learns spatial relationships via attention |\n",
        "# | **Position encoding** | Implicit in CNN | Factored rank+file | Hardcodes grid structure |\n",
        "# | **Value output** | Scalar [-1, +1] | 3 classes [L/D/W] | Sharper gradients, prevents collapse |\n",
        "# | **Parameters** | ~80M | ~4M | Trainable on consumer GPU |\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ## Training Pipeline\n",
        "#\n",
        "# ### Phase 1: Supervised Learning (\"Learn the Rules\")\n",
        "#\n",
        "# ```\n",
        "# Random Games ‚Üí Model predicts moves ‚Üí Cross-entropy loss\n",
        "# ```\n",
        "#\n",
        "# **Goal**: Achieve >95% legal move accuracy\n",
        "#\n",
        "# The model learns:\n",
        "# - How pieces move (bishops diagonal, knights L-shape, etc.)\n",
        "# - Board state encoding/decoding\n",
        "# - Basic position evaluation\n",
        "#\n",
        "# **Data**: ~10k random self-play games (~500k positions)\n",
        "# - Optional: Biased sampling favors captures/checks for richer positions\n",
        "#\n",
        "# **Metrics to watch**:\n",
        "# - `legal_acc`: % of predictions that are legal moves (target: >95%)\n",
        "# - `exact_acc`: % matching the actual move played (~15-20% is fine)\n",
        "# - `grad_norm policy/value`: Should be similar magnitude (within 10√ó)\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ### Phase 2: Self-Play Reinforcement Learning (\"Learn to Win\")\n",
        "#\n",
        "# ```\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ                        Self-Play Loop                           ‚îÇ\n",
        "# ‚îÇ                                                                 ‚îÇ\n",
        "# ‚îÇ  1. GENERATE: Model plays itself using MCTS                     ‚îÇ\n",
        "# ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                 ‚îÇ\n",
        "# ‚îÇ     ‚îÇ  MCTS   ‚îÇ ‚Üê Neural network guides tree search             ‚îÇ\n",
        "# ‚îÇ     ‚îÇ Search  ‚îÇ ‚Üê Explores moves, evaluates positions           ‚îÇ\n",
        "# ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                 ‚îÇ\n",
        "# ‚îÇ          ‚ñº                                                      ‚îÇ\n",
        "# ‚îÇ     Game trajectory: [(board, MCTS_policy, outcome), ...]       ‚îÇ\n",
        "# ‚îÇ                                                                 ‚îÇ\n",
        "# ‚îÇ  2. TRAIN: Update network to match MCTS                         ‚îÇ\n",
        "# ‚îÇ     ‚Ä¢ Policy ‚Üí imitate MCTS visit distribution                  ‚îÇ\n",
        "# ‚îÇ     ‚Ä¢ Value ‚Üí predict game outcome                              ‚îÇ\n",
        "# ‚îÇ                                                                 ‚îÇ\n",
        "# ‚îÇ  3. EVALUATE: New model vs previous best                        ‚îÇ\n",
        "# ‚îÇ     ‚Ä¢ If win rate > 55%: promote to new best                    ‚îÇ\n",
        "# ‚îÇ     ‚Ä¢ Otherwise: keep training                                  ‚îÇ\n",
        "# ‚îÇ                                                                 ‚îÇ\n",
        "# ‚îÇ  4. REPEAT                                                      ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "# ```\n",
        "#\n",
        "# **MCTS (Monte Carlo Tree Search)**:\n",
        "# - Simulates many possible game continuations\n",
        "# - Uses the neural network to evaluate leaf positions\n",
        "# - Balances exploration (try new moves) vs exploitation (play good moves)\n",
        "# - Produces a better policy than the raw network output\n",
        "#\n",
        "# **Anti-Collapse Measures** (critical for small models):\n",
        "# - **Dirichlet noise**: Adds randomness at root to ensure exploration\n",
        "# - **Random opening ply**: Forces random moves in first ~8 ply\n",
        "# - **Temperature schedule**: High temp early (explore) ‚Üí low temp late (exploit)\n",
        "# - **Checkpoint pool**: Plays against diverse past versions, not just self\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ## Reading the Training Output\n",
        "#\n",
        "# ### Phase 1 Output\n",
        "# ```\n",
        "# Grad norms | policy 0.2639 | value 0.2376    ‚Üê Gradient magnitudes (should be similar)\n",
        "# Epoch 010 | loss 1.234 | policy 1.100 | value 0.134 | legal 0.923 | exact 0.152\n",
        "#                                                       ‚ñ≤\n",
        "#                                               Key metric! Target >0.95\n",
        "# ```\n",
        "#\n",
        "# ### Phase 2 Output\n",
        "# ```\n",
        "# Generating 50 self-play games...\n",
        "# Self-play throughput: 45.2 positions/sec     ‚Üê Speed (batched mode is faster)\n",
        "# Generated 3200 positions\n",
        "#\n",
        "# Iter 0 Step 100/500 | policy 2.34 | value 0.89 | grad_p 0.15 | grad_v 0.12\n",
        "#                       ‚ñ≤             ‚ñ≤\n",
        "#                       ‚îÇ             ‚îî‚îÄ‚îÄ Value loss (predicting winner)\n",
        "#                       ‚îî‚îÄ‚îÄ Policy loss (matching MCTS)\n",
        "#\n",
        "# üöÄ New Champion! Win Rate: 62.50%            ‚Üê Model improved!\n",
        "# Champion holds. Win Rate: 45.00%             ‚Üê Model didn't improve this iteration\n",
        "# ```\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ## Expected Training Time (Colab T4)\n",
        "#\n",
        "# | Phase | Duration | Notes |\n",
        "# |-------|----------|-------|\n",
        "# | Data generation (10k games) | ~5 min | One-time |\n",
        "# | Phase 1 (30 epochs) | ~15-20 min | Until legal_acc > 95% |\n",
        "# | Phase 2 (per iteration) | ~10-15 min | Run 10-50 iterations |\n",
        "#\n",
        "# ---\n",
        "#\n",
        "# ## What Success Looks Like\n",
        "#\n",
        "# **Phase 1 complete when**:\n",
        "# - Legal move accuracy > 95%\n",
        "# - Value loss is decreasing (not stuck)\n",
        "# - Gradient norms are balanced (within 10√ó)\n",
        "#\n",
        "# **Phase 2 progress**:\n",
        "# - Win rate vs Phase 1 model increases over iterations\n",
        "# - Self-play games show sensible chess (not random moves)\n",
        "# - Policy loss decreases as model matches MCTS better\n",
        "#\n",
        "# **Final model**:\n",
        "# - Plays legal chess 100% of the time\n",
        "# - Has learned basic tactics (captures hanging pieces)\n",
        "# - Understands piece values and king safety\n",
        "# - Won't beat Stockfish, but plays recognizable chess!"
      ],
      "metadata": {
        "id": "ZMsvGqjXH10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuNGtLJAC0FA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 1. Setup Environment & Dependencies { run: \"auto\" }\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Google Drive Setup (do this FIRST) ---\n",
        "DRIVE_FOLDER = \"TinyAlphaZero\"  # @param {type:\"string\"}\n",
        "RESTORE_FROM_DRIVE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_path = Path(f\"/content/drive/MyDrive/{DRIVE_FOLDER}\")\n",
        "drive_checkpoints = drive_path / \"checkpoints\"\n",
        "\n",
        "# --- Clone/update repository ---\n",
        "REPO_URL = \"https://github.com/tripptytrip/Tiny-AlphaZero\"\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"  # Use absolute path\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\"üì• Cloning repository...\")\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "else:\n",
        "    print(\"üîÑ Updating repository...\")\n",
        "    !cd {REPO_DIR} && git pull --ff-only\n",
        "\n",
        "# --- Restore checkpoints from Google Drive (BEFORE chdir) ---\n",
        "local_checkpoints = Path(REPO_DIR) / \"checkpoints\"\n",
        "\n",
        "if RESTORE_FROM_DRIVE and drive_checkpoints.exists():\n",
        "    checkpoint_files = list(drive_checkpoints.rglob(\"*.pt\"))\n",
        "    if checkpoint_files:\n",
        "        print(f\"\\nüìÇ Restoring {len(checkpoint_files)} checkpoints from Google Drive...\")\n",
        "        for ckpt in checkpoint_files:\n",
        "            dest = local_checkpoints / ckpt.relative_to(drive_checkpoints)\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(ckpt, dest)\n",
        "            print(f\"   ‚úì {dest.name}\")\n",
        "        print(f\"‚úÖ Restored from {drive_checkpoints}\")\n",
        "    else:\n",
        "        print(f\"\\nüìÇ No .pt files found in {drive_checkpoints}\")\n",
        "else:\n",
        "    print(f\"\\nüìÇ No Drive checkpoints to restore (fresh start)\")\n",
        "\n",
        "# Now change to repo directory\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Install dependencies ---\n",
        "!pip install -q chess pyyaml tqdm\n",
        "\n",
        "# --- Verify hardware ---\n",
        "import torch\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"üöÄ GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
        "\n",
        "    if \"T4\" in gpu_name:\n",
        "        RECOMMENDED_BATCH = 512\n",
        "        RECOMMENDED_BATCHED_GAMES = 16\n",
        "    elif \"L4\" in gpu_name or \"A100\" in gpu_name:\n",
        "        RECOMMENDED_BATCH = 1024\n",
        "        RECOMMENDED_BATCHED_GAMES = 32\n",
        "    else:\n",
        "        RECOMMENDED_BATCH = 256\n",
        "        RECOMMENDED_BATCHED_GAMES = 8\n",
        "    print(f\"üìä Recommended batch size: {RECOMMENDED_BATCH}\")\n",
        "    print(f\"üìä Recommended batched games: {RECOMMENDED_BATCHED_GAMES}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU! Go to Runtime > Change runtime type > T4 GPU\")\n",
        "    RECOMMENDED_BATCH = 128\n",
        "    RECOMMENDED_BATCHED_GAMES = 4\n",
        "print(f\"{'='*50}\\n\")\n",
        "\n",
        "# --- Show restored checkpoints ---\n",
        "if local_checkpoints.exists():\n",
        "    all_ckpts = list(local_checkpoints.rglob(\"*.pt\"))\n",
        "    if all_ckpts:\n",
        "        print(\"üìä Current checkpoints:\")\n",
        "        for ckpt in sorted(all_ckpts):\n",
        "            print(f\"   {ckpt.relative_to(local_checkpoints)}\")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(REPO_DIR, \"src\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: Phase 1 - Generate Training Data\n",
        "# =============================================================================\n",
        "\n",
        "# @title 2. Phase 1: Generate Training Data { run: \"auto\" }\n",
        "\n",
        "# --- Data Generation Parameters ---\n",
        "NUM_GAMES = 10000  # @param {type:\"integer\"}\n",
        "MAX_GAME_LEN = 150  # @param {type:\"integer\"}\n",
        "USE_BIASED_SAMPLING = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Biased sampling generates more tactical positions (captures, checks)\n",
        "# This helps the model learn faster than pure random play\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "cmd = f\"python3 {REPO_DIR}/scripts/generate_data.py --num-games {NUM_GAMES} --max-game-length {MAX_GAME_LEN} --output {REPO_DIR}/data/train_games.json\"\n",
        "if USE_BIASED_SAMPLING:\n",
        "    cmd += \" --biased --capture-weight 3.0 --check-weight 2.0\"\n",
        "\n",
        "print(f\"üé≤ Generating {NUM_GAMES} games...\")\n",
        "!{cmd}\n",
        "\n",
        "# Convert to memmap (faster loading during training)\n",
        "print(\"\\nüíæ Converting to memmap format...\")\n",
        "!python3 {REPO_DIR}/scripts/convert_to_memmap.py --input {REPO_DIR}/data/train_games.json --output-dir {REPO_DIR}/data/phase1\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\n‚úÖ Data generation complete in {elapsed:.1f}s\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0kfKix_wC4CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: Phase 1 - Train Supervised Model\n",
        "# =============================================================================\n",
        "\n",
        "# @title 3. Phase 1: Train Supervised Model { run: \"auto\" }\n",
        "\n",
        "# --- Training Parameters ---\n",
        "EPOCHS = 30  # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 512  # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.001  # @param {type:\"number\"}\n",
        "VALUE_WEIGHT = 2.0  # @param {type:\"number\"}\n",
        "\n",
        "# --- Drive Sync ---\n",
        "DRIVE_FOLDER = \"TinyAlphaZero\"  # @param {type:\"string\"}\n",
        "SYNC_TO_DRIVE = True  # @param {type:\"boolean\"}\n",
        "\n",
        "import yaml\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"\n",
        "drive_checkpoints = Path(f\"/content/drive/MyDrive/{DRIVE_FOLDER}/checkpoints\")\n",
        "local_checkpoints = Path(REPO_DIR) / \"checkpoints\"\n",
        "\n",
        "# Update config file with our parameters\n",
        "config_path = f\"{REPO_DIR}/config/phase1.yaml\"\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "config['training']['epochs'] = EPOCHS\n",
        "config['training']['batch_size'] = BATCH_SIZE\n",
        "config['training']['learning_rate'] = LEARNING_RATE\n",
        "config['training']['value_weight'] = VALUE_WEIGHT\n",
        "config['data']['num_workers'] = 2  # Colab has limited CPU cores\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "\n",
        "print(f\"‚öôÔ∏è  Config: epochs={EPOCHS}, batch={BATCH_SIZE}, lr={LEARNING_RATE}, value_weight={VALUE_WEIGHT}\")\n",
        "print(f\"\\nüß† Training Phase 1 Model...\\n\")\n",
        "\n",
        "!cd {REPO_DIR} && python3 scripts/train_phase1.py --data-dir data/phase1 --epochs {EPOCHS}\n",
        "\n",
        "print(\"\\n‚úÖ Phase 1 training complete!\")\n",
        "print(f\"üìÅ Checkpoint saved to: {local_checkpoints}/phase1/best.pt\")\n",
        "\n",
        "# Sync to Drive\n",
        "if SYNC_TO_DRIVE:\n",
        "    phase1_local = local_checkpoints / \"phase1\"\n",
        "    if phase1_local.exists():\n",
        "        for ckpt in phase1_local.glob(\"*.pt\"):\n",
        "            dest = drive_checkpoints / \"phase1\" / ckpt.name\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(ckpt, dest)\n",
        "            print(f\"üíæ Synced: {ckpt.name} ‚Üí Drive\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yBcApHGwC7Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Phase 1: Robust Model Validation (Visual) { run: \"auto\" }\n",
        "\n",
        "import torch\n",
        "import chess\n",
        "import chess.svg\n",
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# --- Setup ---\n",
        "sys.path.insert(0, os.path.join(os.getcwd(), \"src\"))\n",
        "from model.transformer import ChessTransformer\n",
        "from data.encoding import encode_board, encode_move, decode_move\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_path = \"checkpoints/phase1/best.pt\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_model_from_checkpoint(path, device):\n",
        "    if not os.path.exists(path):\n",
        "        return None, f\"‚ùå Checkpoint not found at {path}\"\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(path, map_location=device)\n",
        "        # Attempt to read config from checkpoint, fallback to defaults if missing\n",
        "        model_cfg = ckpt.get('config', {})\n",
        "        # Flatten nested config if it exists (handles both 'model': {...} and flat)\n",
        "        if 'model' in model_cfg: model_cfg = model_cfg['model']\n",
        "\n",
        "        model = ChessTransformer(\n",
        "            vocab_size=model_cfg.get('vocab_size', 23),\n",
        "            num_moves=model_cfg.get('num_moves', 4096),\n",
        "            d_model=model_cfg.get('d_model', 256),\n",
        "            n_layers=model_cfg.get('n_layers', 6),\n",
        "            n_heads=model_cfg.get('n_heads', 8)\n",
        "        ).to(device)\n",
        "\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        model.eval()\n",
        "        return model, \"‚úÖ Model loaded successfully\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Error loading model: {str(e)}\"\n",
        "\n",
        "def analyze_position(model, fen, title):\n",
        "    board = chess.Board(fen)\n",
        "    tokens = torch.tensor(encode_board(board), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        policy, value_logits = model(tokens)\n",
        "        probs = torch.softmax(policy, dim=-1).squeeze()\n",
        "        value = model.get_value(value_logits).item()\n",
        "\n",
        "    # Metrics\n",
        "    legal_indices = [encode_move(m) for m in board.legal_moves]\n",
        "    legal_mass = probs[legal_indices].sum().item()\n",
        "\n",
        "    # Visualization\n",
        "    svg = chess.svg.board(board, size=300)\n",
        "\n",
        "    # HTML Output Construction\n",
        "    html = f\"\"\"\n",
        "    <div style=\"display: flex; align-items: flex-start; margin-bottom: 20px; border: 1px solid #444; padding: 10px; border-radius: 8px;\">\n",
        "        <div style=\"margin-right: 20px;\">{svg}</div>\n",
        "        <div>\n",
        "            <h3>{title}</h3>\n",
        "            <p><strong>Legal Move Probability Mass:</strong>\n",
        "                <span style=\"color: {'lime' if legal_mass > 0.95 else 'orange' if legal_mass > 0.8 else 'red'}\">\n",
        "                {legal_mass:.2%}\n",
        "                </span>\n",
        "            </p>\n",
        "            <p><strong>Value Prediction:</strong> {value:.3f} (Win Prob)</p>\n",
        "            <h4>Top 5 Predictions:</h4>\n",
        "            <ul style=\"list-style-type: none; padding: 0;\">\n",
        "    \"\"\"\n",
        "\n",
        "    top_k = probs.topk(5)\n",
        "    for idx, p in zip(top_k.indices.tolist(), top_k.values.tolist()):\n",
        "        try:\n",
        "            # We use a trick to decode: create a dummy board or just rank/file logic\n",
        "            from_sq, to_sq = idx // 64, idx % 64\n",
        "            move_uci = chess.square_name(from_sq) + chess.square_name(to_sq)\n",
        "\n",
        "            # Check legality\n",
        "            is_legal = idx in legal_indices\n",
        "            icon = \"‚úÖ\" if is_legal else \"‚ùå\"\n",
        "            color = \"#EEE\" if is_legal else \"#F88\"\n",
        "\n",
        "            html += f\"<li style='color: {color}; font-family: monospace;'>{move_uci}: {p:.2%} {icon}</li>\"\n",
        "        except:\n",
        "            html += f\"<li>Error decoding move {idx}</li>\"\n",
        "\n",
        "    html += \"</ul></div></div>\"\n",
        "    return html, legal_mass\n",
        "\n",
        "# --- Execution ---\n",
        "model, status = get_model_from_checkpoint(checkpoint_path, device)\n",
        "print(status)\n",
        "\n",
        "if model:\n",
        "    test_cases = [\n",
        "        (chess.STARTING_FEN, \"1. Starting Position (Opening Knowledge)\"),\n",
        "        (\"rnbqkbnr/pppp1ppp/8/4p3/6P1/5P2/PPPPP2P/RNBQKBNR b KQkq - 0 2\", \"2. Fool's Mate Pattern (Black to Move)\"),\n",
        "        (\"4k3/8/8/8/8/8/8/4R1K1 b - - 0 1\", \"3. Check Evasion (Must Move King)\")\n",
        "    ]\n",
        "\n",
        "    total_mass = 0\n",
        "    full_html = \"<h2>‚ôüÔ∏è Model Diagnostics</h2>\"\n",
        "\n",
        "    for fen, title in test_cases:\n",
        "        html_part, mass = analyze_position(model, fen, title)\n",
        "        full_html += html_part\n",
        "        total_mass += mass\n",
        "\n",
        "    avg_mass = total_mass / len(test_cases)\n",
        "    display(HTML(full_html))\n",
        "\n",
        "    print(f\"{'='*40}\")\n",
        "    print(f\"üìä Average Legal Accuracy: {avg_mass:.2%}\")\n",
        "    if avg_mass > 0.95:\n",
        "        print(\"üéâ PASSED: Model is robust and ready for Phase 2.\")\n",
        "    elif avg_mass > 0.80:\n",
        "        print(\"‚ö†Ô∏è  WARNING: Model is shaky. Phase 2 might be inefficient.\")\n",
        "    else:\n",
        "        print(\"‚ùå FAILED: Do not proceed to Phase 2. Train more on Phase 1.\")"
      ],
      "metadata": {
        "id": "bd9zyRl7W769"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: Phase 2 - Self-Play Training Loop\n",
        "# =============================================================================\n",
        "\n",
        "# @title 5. Phase 2: Self-Play Training Loop { run: \"auto\" }\n",
        "\n",
        "# --- Self-Play Parameters ---\n",
        "NUM_ITERATIONS = 50  # @param {type:\"integer\"}\n",
        "GAMES_PER_ITER = 50  # @param {type:\"integer\"}\n",
        "MCTS_SIMS = 400  # @param {type:\"integer\"}\n",
        "TRAINING_STEPS = 500  # @param {type:\"integer\"}\n",
        "\n",
        "# --- Performance Tuning ---\n",
        "USE_BATCHED = True  # @param {type:\"boolean\"}\n",
        "BATCHED_GAMES = 32  # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 1024  # @param {type:\"integer\"}\n",
        "\n",
        "# --- Anti-Collapse Settings ---\n",
        "DIRICHLET_ALPHA = 0.3  # @param {type:\"number\"}\n",
        "DIRICHLET_FRAC = 0.25  # @param {type:\"number\"}\n",
        "RANDOM_OPENING_PLY = 8  # @param {type:\"integer\"}\n",
        "RANDOM_OPENING_PROB = 0.5  # @param {type:\"number\"}\n",
        "\n",
        "# --- Drive Sync ---\n",
        "DRIVE_FOLDER = \"TinyAlphaZero\"  # @param {type:\"string\"}\n",
        "SAVE_TO_DRIVE_EVERY = 1  # @param {type:\"integer\"}\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"\n",
        "drive_checkpoints = Path(f\"/content/drive/MyDrive/{DRIVE_FOLDER}/checkpoints\")\n",
        "local_checkpoints = Path(REPO_DIR) / \"checkpoints\"\n",
        "\n",
        "def sync_to_drive():\n",
        "    \"\"\"Copy local checkpoints to Google Drive.\"\"\"\n",
        "    if not local_checkpoints.exists():\n",
        "        return\n",
        "    for ckpt in local_checkpoints.rglob(\"*.pt\"):\n",
        "        dest = drive_checkpoints / ckpt.relative_to(local_checkpoints)\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(ckpt, dest)\n",
        "    print(f\"üíæ Synced checkpoints to Google Drive\")\n",
        "\n",
        "def find_latest_checkpoint():\n",
        "    \"\"\"Find the most recent checkpoint (prefer phase2, fall back to phase1).\"\"\"\n",
        "    phase2_ckpts = sorted(local_checkpoints.glob(\"phase2/phase2_iter_*.pt\"))\n",
        "    if phase2_ckpts:\n",
        "        return str(phase2_ckpts[-1])\n",
        "\n",
        "    phase1_best = local_checkpoints / \"phase1\" / \"best.pt\"\n",
        "    if phase1_best.exists():\n",
        "        return str(phase1_best)\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_starting_iteration():\n",
        "    \"\"\"Determine which iteration to start from based on existing checkpoints.\"\"\"\n",
        "    phase2_ckpts = sorted(local_checkpoints.glob(\"phase2/phase2_iter_*.pt\"))\n",
        "    if phase2_ckpts:\n",
        "        last_ckpt = phase2_ckpts[-1].stem  # \"phase2_iter_5\"\n",
        "        last_iter = int(last_ckpt.split(\"_\")[-1])\n",
        "        return last_iter + 1\n",
        "    return 0\n",
        "\n",
        "# Check for starting checkpoint\n",
        "checkpoint = find_latest_checkpoint()\n",
        "start_iteration = get_starting_iteration()\n",
        "\n",
        "if checkpoint is None:\n",
        "    print(\"‚ùå No checkpoint found! Run Phase 1 first.\")\n",
        "else:\n",
        "    print(f\"üöÄ Starting Phase 2 Self-Play Training\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Starting from: {Path(checkpoint).name}\")\n",
        "    print(f\"Starting iteration: {start_iteration}\")\n",
        "    print(f\"Target iterations: {NUM_ITERATIONS}\")\n",
        "    print(f\"Games/iter: {GAMES_PER_ITER} | MCTS sims: {MCTS_SIMS}\")\n",
        "    print(f\"Batched: {USE_BATCHED} ({BATCHED_GAMES} parallel)\")\n",
        "    print(f\"Auto-save to Drive: every {SAVE_TO_DRIVE_EVERY} iteration(s)\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    for iteration in range(start_iteration, NUM_ITERATIONS):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üîÑ ITERATION {iteration + 1}/{NUM_ITERATIONS}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Find latest checkpoint for this iteration\n",
        "        ckpt = find_latest_checkpoint()\n",
        "\n",
        "        cmd = f\"\"\"cd {REPO_DIR} && python3 scripts/train_phase2.py \\\n",
        "            --checkpoint {ckpt} \\\n",
        "            --num-games {GAMES_PER_ITER} \\\n",
        "            --mcts-sims {MCTS_SIMS} \\\n",
        "            --training-steps {TRAINING_STEPS} \\\n",
        "            --batch-size {BATCH_SIZE} \\\n",
        "            --dirichlet-alpha {DIRICHLET_ALPHA} \\\n",
        "            --dirichlet-frac {DIRICHLET_FRAC} \\\n",
        "            --random-opening-ply {RANDOM_OPENING_PLY} \\\n",
        "            --random-opening-prob {RANDOM_OPENING_PROB}\"\"\"\n",
        "\n",
        "        if USE_BATCHED:\n",
        "            cmd += f\" --use-batched --batched-games {BATCHED_GAMES}\"\n",
        "\n",
        "        !{cmd}\n",
        "\n",
        "        # Sync to Google Drive periodically\n",
        "        if (iteration + 1) % SAVE_TO_DRIVE_EVERY == 0:\n",
        "            sync_to_drive()\n",
        "\n",
        "    # Final sync\n",
        "    sync_to_drive()\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"‚úÖ Phase 2 Training Complete!\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "peO2FyVWOA5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 6: Manual Save to Google Drive (if needed)\n",
        "# =============================================================================\n",
        "\n",
        "# @title 6. Save Model to Google Drive (Manual) { run: \"auto\" }\n",
        "\n",
        "DRIVE_FOLDER = \"TinyAlphaZero\"  # @param {type:\"string\"}\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"\n",
        "drive_checkpoints = Path(f\"/content/drive/MyDrive/{DRIVE_FOLDER}/checkpoints\")\n",
        "local_checkpoints = Path(REPO_DIR) / \"checkpoints\"\n",
        "\n",
        "if local_checkpoints.exists():\n",
        "    count = 0\n",
        "    for ckpt in local_checkpoints.rglob(\"*.pt\"):\n",
        "        dest = drive_checkpoints / ckpt.relative_to(local_checkpoints)\n",
        "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy2(ckpt, dest)\n",
        "        print(f\"üìÅ Saved: {ckpt.relative_to(local_checkpoints)}\")\n",
        "        count += 1\n",
        "    print(f\"\\n‚úÖ {count} checkpoints saved to Google Drive: {drive_checkpoints}\")\n",
        "else:\n",
        "    print(\"‚ùå No local checkpoints found\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z98z-TAIaBEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Benchmark & Analyze Model { run: \"auto\" }\n",
        "\n",
        "# --- Configuration ---\n",
        "NUM_ARENA_GAMES = 20  # @param {type:\"integer\"}\n",
        "MCTS_SIMS = 200  # @param {type:\"integer\"}\n",
        "SHOW_SAMPLE_GAME = True  # @param {type:\"boolean\"}\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = \"/content/Tiny-AlphaZero\"\n",
        "sys.path.insert(0, f\"{REPO_DIR}/src\")\n",
        "\n",
        "import torch\n",
        "import chess\n",
        "import chess.svg\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "\n",
        "from model.transformer import ChessTransformer\n",
        "from data.encoding import encode_board, encode_move, decode_move\n",
        "from mcts.tree import MCTS, MCTSConfig\n",
        "\n",
        "# --- Find Best Checkpoint ---\n",
        "local_checkpoints = Path(REPO_DIR) / \"checkpoints\"\n",
        "\n",
        "def find_best_checkpoint():\n",
        "    \"\"\"Find the most recent/best checkpoint.\"\"\"\n",
        "    # Priority: best_generation > phase2_iter > phase1/best\n",
        "    for pattern in [\"phase2/best_generation_*.pt\", \"phase2/phase2_iter_*.pt\", \"phase1/best.pt\"]:\n",
        "        matches = sorted(local_checkpoints.glob(pattern))\n",
        "        if matches:\n",
        "            return matches[-1]\n",
        "    return None\n",
        "\n",
        "checkpoint_path = find_best_checkpoint()\n",
        "\n",
        "if checkpoint_path is None:\n",
        "    print(\"‚ùå No checkpoints found. Run training first.\")\n",
        "else:\n",
        "    print(f\"üìä TinyAlphaZero Benchmark Suite\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Checkpoint: {checkpoint_path.relative_to(local_checkpoints)}\")\n",
        "\n",
        "    # --- Load Model ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ChessTransformer().to(device)\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device)[\"model_state_dict\"])\n",
        "    model.eval()\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # TEST 1: Legal Move Accuracy\n",
        "    # =========================================================================\n",
        "    print(\"üß™ TEST 1: Legal Move Accuracy\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    test_positions = [\n",
        "        (chess.STARTING_FEN, \"Starting position\"),\n",
        "        (\"rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1\", \"After 1.e4\"),\n",
        "        (\"r1bqkb1r/pppp1ppp/2n2n2/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R w KQkq - 4 4\", \"Italian Game\"),\n",
        "        (\"rnbqkb1r/pp2pppp/2p2n2/3p4/2PP4/2N5/PP2PPPP/R1BQKBNR w KQkq - 0 4\", \"Slav Defense\"),\n",
        "        (\"r1bqr1k1/ppp2ppp/2np1n2/2b1p3/2B1P3/2NP1N2/PPP2PPP/R1BQR1K1 w - - 0 8\", \"Middlegame\"),\n",
        "        (\"8/8/4k3/8/8/4K3/4P3/8 w - - 0 1\", \"King + Pawn endgame\"),\n",
        "        (\"r3k2r/pppppppp/8/8/8/8/PPPPPPPP/R3K2R w KQkq - 0 1\", \"Castling available\"),\n",
        "    ]\n",
        "\n",
        "    total_legal_mass = 0\n",
        "    for fen, name in test_positions:\n",
        "        board = chess.Board(fen)\n",
        "        tokens = torch.tensor(encode_board(board), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy, value_logits = model(tokens)\n",
        "            probs = torch.softmax(policy, dim=-1).squeeze()\n",
        "            value = model.get_value(value_logits).item()\n",
        "\n",
        "        legal_indices = [encode_move(m) for m in board.legal_moves]\n",
        "        legal_mass = probs[legal_indices].sum().item()\n",
        "        total_legal_mass += legal_mass\n",
        "\n",
        "        # Top move\n",
        "        top_idx = probs.argmax().item()\n",
        "        top_move = decode_move(top_idx, board)\n",
        "        top_legal = \"‚úÖ\" if top_idx in legal_indices else \"‚ùå\"\n",
        "\n",
        "        print(f\"  {name:25s} | Legal: {legal_mass:6.2%} | Value: {value:+.3f} | Top: {top_move.uci()} {top_legal}\")\n",
        "\n",
        "    avg_legal = total_legal_mass / len(test_positions)\n",
        "    print(f\"\\n  Average legal move mass: {avg_legal:.2%}\")\n",
        "\n",
        "    if avg_legal > 0.98:\n",
        "        print(\"  ‚úÖ Excellent - model has mastered legal moves\")\n",
        "    elif avg_legal > 0.95:\n",
        "        print(\"  ‚úÖ Good - ready for self-play\")\n",
        "    elif avg_legal > 0.90:\n",
        "        print(\"  ‚ö†Ô∏è  Okay - may need more Phase 1 training\")\n",
        "    else:\n",
        "        print(\"  ‚ùå Poor - needs more Phase 1 training\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # TEST 2: Arena vs Random Player\n",
        "    # =========================================================================\n",
        "    print(f\"\\nüß™ TEST 2: Arena vs Random Player ({NUM_ARENA_GAMES} games)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    import random\n",
        "\n",
        "    mcts = MCTS(model, config=MCTSConfig(num_simulations=MCTS_SIMS), device=device)\n",
        "\n",
        "    wins, losses, draws = 0, 0, 0\n",
        "    total_moves = 0\n",
        "\n",
        "    for game_idx in range(NUM_ARENA_GAMES):\n",
        "        board = chess.Board()\n",
        "        model_is_white = (game_idx % 2 == 0)  # Alternate colors\n",
        "\n",
        "        while not board.is_game_over() and board.fullmove_number < 150:\n",
        "            if board.turn == chess.WHITE:\n",
        "                is_model_turn = model_is_white\n",
        "            else:\n",
        "                is_model_turn = not model_is_white\n",
        "\n",
        "            if is_model_turn:\n",
        "                # Model plays with MCTS\n",
        "                move_idx = mcts.select_move(board, temperature=0.1)\n",
        "                move = decode_move(move_idx, board)\n",
        "            else:\n",
        "                # Random player\n",
        "                move = random.choice(list(board.legal_moves))\n",
        "\n",
        "            board.push(move)\n",
        "            total_moves += 1\n",
        "\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            if model_is_white:\n",
        "                wins += 1\n",
        "            else:\n",
        "                losses += 1\n",
        "        elif result == \"0-1\":\n",
        "            if model_is_white:\n",
        "                losses += 1\n",
        "            else:\n",
        "                wins += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "        # Progress indicator\n",
        "        print(f\"  Game {game_idx + 1:2d}/{NUM_ARENA_GAMES}: {result:7s} | Model as {'White' if model_is_white else 'Black'} | Moves: {board.fullmove_number}\")\n",
        "\n",
        "    win_rate = wins / NUM_ARENA_GAMES\n",
        "    print(f\"\\n  Results: {wins}W - {draws}D - {losses}L\")\n",
        "    print(f\"  Win rate: {win_rate:.1%}\")\n",
        "    print(f\"  Avg game length: {total_moves / NUM_ARENA_GAMES:.1f} ply\")\n",
        "\n",
        "    if win_rate > 0.95:\n",
        "        print(\"  üèÜ Excellent - dominates random play\")\n",
        "    elif win_rate > 0.85:\n",
        "        print(\"  ‚úÖ Very good - strong tactical understanding\")\n",
        "    elif win_rate > 0.70:\n",
        "        print(\"  ‚úÖ Good - solid progress\")\n",
        "    elif win_rate > 0.50:\n",
        "        print(\"  ‚ö†Ô∏è  Okay - learning but needs more training\")\n",
        "    else:\n",
        "        print(\"  ‚ùå Poor - not better than random yet\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # TEST 3: Tactical Puzzles\n",
        "    # =========================================================================\n",
        "    print(f\"\\nüß™ TEST 3: Tactical Puzzles\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    puzzles = [\n",
        "        # (FEN, best_move_uci, description)\n",
        "        (\"r1bqkb1r/pppp1ppp/2n2n2/4p2Q/2B1P3/8/PPPP1PPP/RNB1K1NR w KQkq - 4 4\", \"h5f7\", \"Scholar's Mate\"),\n",
        "        (\"r1b1k2r/ppppqppp/2n2n2/2b1p3/2B1P3/3P1N2/PPP2PPP/RNBQK2R w KQkq - 0 6\", \"c4f7\", \"Fork on f7\"),\n",
        "        (\"rnbqkbnr/ppp2ppp/4p3/3pP3/3P4/8/PPP2PPP/RNBQKBNR w KQkq d6 0 4\", \"e5d6\", \"En passant capture\"),\n",
        "        (\"r3k2r/pppppppp/8/8/8/8/PPPPPPPP/R3K2R w KQkq - 0 1\", \"e1g1\", \"Kingside castle\"),\n",
        "        (\"8/5P2/8/8/8/8/8/4K2k w - - 0 1\", \"f7f8q\", \"Pawn promotion\"),\n",
        "    ]\n",
        "\n",
        "    correct = 0\n",
        "    for fen, best_uci, description in puzzles:\n",
        "        board = chess.Board(fen)\n",
        "        tokens = torch.tensor(encode_board(board), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            policy, _ = model(tokens)\n",
        "            probs = torch.softmax(policy, dim=-1).squeeze()\n",
        "\n",
        "        # Mask illegal moves\n",
        "        legal_indices = [encode_move(m) for m in board.legal_moves]\n",
        "        masked_probs = torch.zeros_like(probs)\n",
        "        masked_probs[legal_indices] = probs[legal_indices]\n",
        "\n",
        "        top_idx = masked_probs.argmax().item()\n",
        "        top_move = decode_move(top_idx, board)\n",
        "\n",
        "        best_move = chess.Move.from_uci(best_uci)\n",
        "        is_correct = (top_move == best_move)\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        status = \"‚úÖ\" if is_correct else f\"‚ùå (played {top_move.uci()})\"\n",
        "        print(f\"  {description:20s} | Best: {best_uci} | {status}\")\n",
        "\n",
        "    puzzle_score = correct / len(puzzles)\n",
        "    print(f\"\\n  Puzzle score: {correct}/{len(puzzles)} ({puzzle_score:.0%})\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # TEST 4: Sample Game Display\n",
        "    # =========================================================================\n",
        "    if SHOW_SAMPLE_GAME:\n",
        "        print(f\"\\nüß™ TEST 4: Sample Game (Model vs Random)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        board = chess.Board()\n",
        "        moves = []\n",
        "\n",
        "        while not board.is_game_over() and board.fullmove_number <= 40:\n",
        "            if board.turn == chess.WHITE:\n",
        "                # Model plays white with MCTS\n",
        "                move_idx = mcts.select_move(board, temperature=0.3)\n",
        "                move = decode_move(move_idx, board)\n",
        "            else:\n",
        "                # Random plays black\n",
        "                move = random.choice(list(board.legal_moves))\n",
        "\n",
        "            moves.append(move.uci())\n",
        "            board.push(move)\n",
        "\n",
        "        # Print moves in PGN-ish format\n",
        "        print(\"  Moves:\")\n",
        "        move_str = \"\"\n",
        "        for i, uci in enumerate(moves):\n",
        "            if i % 2 == 0:\n",
        "                move_str += f\"  {i//2 + 1}. {uci}\"\n",
        "            else:\n",
        "                move_str += f\" {uci}\"\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(move_str)\n",
        "                move_str = \"\"\n",
        "        if move_str:\n",
        "            print(move_str)\n",
        "\n",
        "        print(f\"\\n  Result: {board.result()}\")\n",
        "        print(f\"  Final position FEN: {board.fen()}\")\n",
        "\n",
        "        # Display final board\n",
        "        try:\n",
        "            svg = chess.svg.board(board=board, size=350)\n",
        "            display(HTML(f\"<div style='margin: 20px 0;'>{svg}</div>\"))\n",
        "        except:\n",
        "            print(\"  (SVG display not available)\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # SUMMARY\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üìä BENCHMARK SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Legal move accuracy:  {avg_legal:.1%}\")\n",
        "    print(f\"  Win rate vs random:   {win_rate:.1%}\")\n",
        "    print(f\"  Tactical puzzles:     {correct}/{len(puzzles)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Overall assessment\n",
        "    score = (avg_legal * 0.3) + (win_rate * 0.5) + (puzzle_score * 0.2)\n",
        "    if score > 0.90:\n",
        "        print(\"üèÜ Overall: Excellent! Model plays strong chess.\")\n",
        "    elif score > 0.75:\n",
        "        print(\"‚úÖ Overall: Good progress. Continue Phase 2 training.\")\n",
        "    elif score > 0.60:\n",
        "        print(\"‚ö†Ô∏è  Overall: Learning. Needs more iterations.\")\n",
        "    else:\n",
        "        print(\"‚ùå Overall: Early stage. Keep training.\")"
      ],
      "metadata": {
        "id": "TtaWd1waDHJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "75fc1cbc-e78d-4de7-86bc-10dc0f26a87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä TinyAlphaZero Benchmark Suite\n",
            "============================================================\n",
            "Checkpoint: phase2/best_generation_0.pt\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "üß™ TEST 1: Legal Move Accuracy\n",
            "----------------------------------------\n",
            "  Starting position         | Legal: 99.86% | Value: -0.229 | Top: a2a4 ‚úÖ\n",
            "  After 1.e4                | Legal: 99.95% | Value: +0.213 | Top: c7c5 ‚úÖ\n",
            "  Italian Game              | Legal: 97.50% | Value: -0.000 | Top: f3e5 ‚úÖ\n",
            "  Slav Defense              | Legal: 96.38% | Value: +0.001 | Top: g2g3 ‚úÖ\n",
            "  Middlegame                | Legal: 95.44% | Value: -0.000 | Top: c4d5 ‚úÖ\n",
            "  King + Pawn endgame       | Legal: 96.28% | Value: +0.000 | Top: e3f3 ‚úÖ\n",
            "  Castling available        | Legal: 99.76% | Value: +0.403 | Top: f2f3 ‚úÖ\n",
            "\n",
            "  Average legal move mass: 97.88%\n",
            "  ‚úÖ Good - ready for self-play\n",
            "\n",
            "üß™ TEST 2: Arena vs Random Player (20 games)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}